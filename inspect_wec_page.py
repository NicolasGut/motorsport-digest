#!/usr/bin/env python3
"""
WEC Page Inspector
Outil pour analyser la structure de la page WEC et ajuster le scraper

Usage: python inspect_wec_page.py
"""

import requests
from bs4 import BeautifulSoup
import json
import sys

def inspect_wec_page():
    """Inspecter la page WEC en d√©tail"""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
        'Referer': 'https://www.google.com/',
    }
    
    url = 'https://www.fiawec.com/fr/page/news/30'
    
    print("=" * 80)
    print("WEC PAGE INSPECTOR - Analyse d√©taill√©e")
    print("=" * 80)
    print(f"\nüìç URL: {url}\n")
    
    try:
        print("‚è≥ Fetching page...")
        response = requests.get(url, headers=headers, timeout=20)
        
        print(f"‚úÖ Status: {response.status_code}")
        print(f"üì¶ Content-Type: {response.headers.get('content-type')}")
        print(f"üìè Size: {len(response.content):,} bytes\n")
        
        if response.status_code != 200:
            print(f"‚ùå HTTP Error {response.status_code}")
            return
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ============================================
        # 1. STRUCTURE G√âN√âRALE
        # ============================================
        
        print("=" * 80)
        print("1Ô∏è‚É£  STRUCTURE G√âN√âRALE")
        print("=" * 80)
        
        print(f"\nüìä Balises principales:")
        print(f"   ‚Ä¢ <article>: {len(soup.find_all('article'))}")
        print(f"   ‚Ä¢ <section>: {len(soup.find_all('section'))}")
        print(f"   ‚Ä¢ <div>: {len(soup.find_all('div'))}")
        print(f"   ‚Ä¢ <a>: {len(soup.find_all('a', href=True))}")
        
        # ============================================
        # 2. RECHERCHE ARTICLES
        # ============================================
        
        print("\n" + "=" * 80)
        print("2Ô∏è‚É£  RECHERCHE ARTICLES DE NEWS")
        print("=" * 80)
        
        # Strat√©gie 1 : Articles
        articles = soup.find_all('article')
        print(f"\nüì∞ Strat√©gie 1 - <article>: {len(articles)} trouv√©s")
        if articles:
            for i, art in enumerate(articles[:2], 1):
                title = art.find(['h1', 'h2', 'h3', 'h4'])
                if title:
                    print(f"   {i}. {title.get_text(strip=True)[:70]}...")
        
        # Strat√©gie 2 : Divs avec classes sp√©cifiques
        print(f"\nüì∞ Strat√©gie 2 - Divs avec classes:")
        keywords = ['news', 'article', 'post', 'item', 'card', 'tile', 'entry', 'story']
        for keyword in keywords:
            divs = soup.find_all('div', class_=lambda x: x and keyword in ' '.join(x).lower())
            if divs:
                print(f"   ‚Ä¢ '{keyword}': {len(divs)} divs")
                if divs[:1]:
                    classes = divs[0].get('class')
                    print(f"      Classes: {classes}")
        
        # Strat√©gie 3 : Liens vers news
        news_links = soup.find_all('a', href=lambda x: x and ('/news/' in x or '/article/' in x))
        print(f"\nüîó Strat√©gie 3 - Liens /news/ ou /article/: {len(news_links)}")
        if news_links:
            print(f"\n   √âchantillons:")
            for i, link in enumerate(news_links[:5], 1):
                href = link.get('href')
                text = link.get_text(strip=True)
                if text and len(text) > 10:
                    print(f"   {i}. {text[:60]}...")
                    print(f"      ‚Üí {href}")
        
        # ============================================
        # 3. ANALYSE JSON / NEXT.JS
        # ============================================
        
        print("\n" + "=" * 80)
        print("3Ô∏è‚É£  DONN√âES JAVASCRIPT / JSON")
        print("=" * 80)
        
        # Next.js data
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            print("\n‚úÖ Next.js d√©tect√© (__NEXT_DATA__)")
            try:
                data = json.loads(next_data.string)
                print(f"   Keys: {list(data.keys())}")
                
                if 'props' in data and 'pageProps' in data['props']:
                    page_props = data['props']['pageProps']
                    print(f"   pageProps keys: {list(page_props.keys())[:10]}")
                    
                    # Chercher donn√©es d'articles
                    if 'news' in page_props:
                        print(f"\n   üéØ TROUV√â 'news' dans pageProps!")
                        news_data = page_props['news']
                        if isinstance(news_data, list):
                            print(f"      Type: Liste de {len(news_data)} articles")
                            if news_data:
                                print(f"      Premier article keys: {list(news_data[0].keys())[:10]}")
                        elif isinstance(news_data, dict):
                            print(f"      Type: Dict avec keys: {list(news_data.keys())}")
                    
                    # Chercher autres donn√©es possibles
                    for key in page_props.keys():
                        if 'article' in key.lower() or 'post' in key.lower() or 'item' in key.lower():
                            print(f"   üîç Cl√© int√©ressante: '{key}' = {type(page_props[key])}")
            
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur parsing JSON: {e}")
        else:
            print("\n‚ö†Ô∏è  Pas de __NEXT_DATA__ (pas Next.js)")
        
        # Autres scripts JSON
        json_scripts = soup.find_all('script', type='application/json')
        if json_scripts:
            print(f"\nüìú Scripts JSON trouv√©s: {len(json_scripts)}")
        
        # ============================================
        # 4. RECOMMANDATIONS
        # ============================================
        
        print("\n" + "=" * 80)
        print("4Ô∏è‚É£  RECOMMANDATIONS")
        print("=" * 80)
        
        print("\nüí° Bas√© sur l'analyse:")
        
        if next_data:
            print("   ‚úÖ Site Next.js d√©tect√©")
            print("   ‚Üí Meilleure strat√©gie: Extraire donn√©es de __NEXT_DATA__")
            print("   ‚Üí Code √† ajouter dans web_scraper.py")
        elif articles:
            print("   ‚úÖ Balises <article> trouv√©es")
            print("   ‚Üí Strat√©gie actuelle devrait fonctionner")
        elif news_links:
            print("   ‚úÖ Liens /news/ trouv√©s")
            print("   ‚Üí Utiliser extraction par liens")
        else:
            print("   ‚ö†Ô∏è  Structure non standard d√©tect√©e")
            print("   ‚Üí Inspection manuelle n√©cessaire")
        
        # ============================================
        # 5. SAUVEGARDER HTML
        # ============================================
        
        print("\n" + "=" * 80)
        print("5Ô∏è‚É£  SAUVEGARDE")
        print("=" * 80)
        
        output_file = 'wec_page_debug.html'
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        print(f"\nüíæ Page HTML sauvegard√©e: {output_file}")
        print("   ‚Üí Ouvrez ce fichier dans un navigateur pour inspection visuelle")
        print("   ‚Üí Ou utilisez DevTools pour voir la structure compl√®te")
        
        print("\n" + "=" * 80)
        print("‚úÖ INSPECTION TERMIN√âE")
        print("=" * 80)
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    inspect_wec_page()
